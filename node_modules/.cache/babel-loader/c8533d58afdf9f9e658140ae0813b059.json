{"ast":null,"code":"import _taggedTemplateLiteral from\"/Users/vicayu/Desktop/Judging-the-Judges.github.io/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/taggedTemplateLiteral.js\";var _templateObject;import React from\"react\";import AnimationRevealPage from\"helpers/AnimationRevealPage.js\";import styled from\"styled-components\";import Header from\"components/headers/light.js\";import Footer from\"components/footers/MiniCenteredFooter.js\";import{Container,ContentWithPaddingXl}from\"components/misc/Layouts\";import{SectionHeading,Subheading as SubheadingBase}from\"components/misc/Headings\";import{jsx as _jsx,jsxs as _jsxs}from\"react/jsx-runtime\";const HeadingRow=styled.div({\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\"});const Heading=styled(SectionHeading)({\"--tw-text-opacity\":\"1\",\"color\":\"rgb(26 32 44 / var(--tw-text-opacity))\"});const Subheading=styled(SubheadingBase)({\"marginBottom\":\"1rem\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(100 21 255 / var(--tw-text-opacity))\"});const Description=styled.p({\"marginTop\":\"1rem\",\"maxWidth\":\"42rem\",\"textAlign\":\"center\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(113 128 150 / var(--tw-text-opacity))\"});const SectionTitle=styled.h2({\"marginTop\":\"4rem\",\"marginBottom\":\"1.5rem\",\"fontSize\":\"1.5rem\",\"fontWeight\":\"700\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(26 32 44 / var(--tw-text-opacity))\"});const FindingsGrid=styled.div({\"marginTop\":\"2rem\",\"display\":\"grid\",\"gridTemplateColumns\":\"repeat(1, minmax(0, 1fr))\",\"gap\":\"2rem\",\"@media (min-width: 768px)\":{\"gridTemplateColumns\":\"repeat(2, minmax(0, 1fr))\"}});const FindingCard=styled.div({\"borderRadius\":\"0.75rem\",\"borderTopWidth\":\"4px\",\"--tw-border-opacity\":\"1\",\"borderColor\":\"rgb(100 21 255 / var(--tw-border-opacity))\",\"--tw-bg-opacity\":\"1\",\"backgroundColor\":\"rgb(255 255 255 / var(--tw-bg-opacity))\",\"padding\":\"1.5rem\",\"--tw-shadow\":\"0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05)\",\"--tw-shadow-colored\":\"0 10px 15px -3px var(--tw-shadow-color), 0 4px 6px -2px var(--tw-shadow-color)\",\"boxShadow\":\"var(--tw-ring-offset-shadow, 0 0 #0000), var(--tw-ring-shadow, 0 0 #0000), var(--tw-shadow)\"});const FindingTitle=styled.h3({\"marginBottom\":\"0.75rem\",\"fontSize\":\"1.125rem\",\"fontWeight\":\"700\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(26 32 44 / var(--tw-text-opacity))\"});const FindingText=styled.p({\"lineHeight\":\"1.625\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(113 128 150 / var(--tw-text-opacity))\"});const ChartPlaceholder=styled.div({\"marginTop\":\"2rem\",\"display\":\"flex\",\"height\":\"16rem\",\"alignItems\":\"center\",\"justifyContent\":\"center\",\"borderRadius\":\"0.75rem\",\"--tw-bg-opacity\":\"1\",\"backgroundColor\":\"rgb(247 250 252 / var(--tw-bg-opacity))\",\"padding\":\"2rem\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(160 174 192 / var(--tw-text-opacity))\"});const InsightBox=styled.div({\"marginTop\":\"2rem\",\"borderRadius\":\"0.75rem\",\"--tw-bg-opacity\":\"1\",\"backgroundColor\":\"rgb(162 115 255 / var(--tw-bg-opacity))\",\"padding\":\"1.5rem\"});const InsightTitle=styled.h4({\"marginBottom\":\"0.5rem\",\"fontWeight\":\"700\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(80 17 204 / var(--tw-text-opacity))\"});const InsightText=styled.p({\"--tw-text-opacity\":\"1\",\"color\":\"rgb(90 19 230 / var(--tw-text-opacity))\"});const MetricRow=styled.div({\"marginTop\":\"2rem\",\"display\":\"flex\",\"flexWrap\":\"wrap\",\"justifyContent\":\"center\",\"gap\":\"1.5rem\"});const MetricCard=styled.div(_templateObject||(_templateObject=_taggedTemplateLiteral([\"\\n  \",\"\\n\"])),{\"minWidth\":\"200px\",\"borderRadius\":\"0.75rem\",\"--tw-bg-opacity\":\"1\",\"backgroundColor\":\"rgb(255 255 255 / var(--tw-bg-opacity))\",\"padding\":\"1.5rem\",\"textAlign\":\"center\",\"--tw-shadow\":\"0 1px 3px 0 rgba(0, 0, 0, 0.1), 0 1px 2px 0 rgba(0, 0, 0, 0.06)\",\"--tw-shadow-colored\":\"0 1px 3px 0 var(--tw-shadow-color), 0 1px 2px 0 var(--tw-shadow-color)\",\"boxShadow\":\"var(--tw-ring-offset-shadow, 0 0 #0000), var(--tw-ring-shadow, 0 0 #0000), var(--tw-shadow)\"});const MetricValue=styled.div({\"fontSize\":\"1.875rem\",\"fontWeight\":\"700\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(100 21 255 / var(--tw-text-opacity))\"});const MetricLabel=styled.div({\"marginTop\":\"0.5rem\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(113 128 150 / var(--tw-text-opacity))\"});export default()=>{return/*#__PURE__*/_jsxs(AnimationRevealPage,{children:[/*#__PURE__*/_jsx(Header,{}),/*#__PURE__*/_jsx(Container,{children:/*#__PURE__*/_jsxs(ContentWithPaddingXl,{children:[/*#__PURE__*/_jsxs(HeadingRow,{children:[/*#__PURE__*/_jsx(Subheading,{children:\"Findings\"}),/*#__PURE__*/_jsx(Heading,{children:\"Results & Analysis\"}),/*#__PURE__*/_jsx(Description,{children:\"Comprehensive evaluation results and key findings from our LLM-as-a-Judge research.\"})]}),/*#__PURE__*/_jsxs(MetricRow,{children:[/*#__PURE__*/_jsxs(MetricCard,{children:[/*#__PURE__*/_jsx(MetricValue,{children:\"87.3%\"}),/*#__PURE__*/_jsx(MetricLabel,{children:\"Average Accuracy\"})]}),/*#__PURE__*/_jsxs(MetricCard,{children:[/*#__PURE__*/_jsx(MetricValue,{children:\"15.2%\"}),/*#__PURE__*/_jsx(MetricLabel,{children:\"Position Bias Rate\"})]}),/*#__PURE__*/_jsxs(MetricCard,{children:[/*#__PURE__*/_jsx(MetricValue,{children:\"0.82\"}),/*#__PURE__*/_jsx(MetricLabel,{children:\"Inter-model Agreement\"})]}),/*#__PURE__*/_jsxs(MetricCard,{children:[/*#__PURE__*/_jsx(MetricValue,{children:\"23%\"}),/*#__PURE__*/_jsx(MetricLabel,{children:\"Length Bias Impact\"})]})]}),/*#__PURE__*/_jsx(SectionTitle,{children:\"Key Findings\"}),/*#__PURE__*/_jsxs(FindingsGrid,{children:[/*#__PURE__*/_jsxs(FindingCard,{children:[/*#__PURE__*/_jsx(FindingTitle,{children:\"Position Bias\"}),/*#__PURE__*/_jsx(FindingText,{children:\"Most LLM judges exhibit position bias, showing preference for responses appearing in certain positions. GPT-4 and Claude-3 demonstrate the most stability in this regard.\"})]}),/*#__PURE__*/_jsxs(FindingCard,{children:[/*#__PURE__*/_jsx(FindingTitle,{children:\"Length Preference\"}),/*#__PURE__*/_jsx(FindingText,{children:\"Longer responses generally receive higher scores, even when content quality is similar. This bias is particularly pronounced when evaluating creative writing tasks.\"})]}),/*#__PURE__*/_jsxs(FindingCard,{children:[/*#__PURE__*/_jsx(FindingTitle,{children:\"Self-Enhancement Bias\"}),/*#__PURE__*/_jsx(FindingText,{children:\"LLM judges tend to give higher scores to content they generated themselves. This self-enhancement bias reaches up to 12% in some models.\"})]}),/*#__PURE__*/_jsxs(FindingCard,{children:[/*#__PURE__*/_jsx(FindingTitle,{children:\"Consistency\"}),/*#__PURE__*/_jsx(FindingText,{children:\"When repeatedly evaluating the same content, consistency varies significantly across models. Top-tier models achieve over 90% self-consistency.\"})]})]}),/*#__PURE__*/_jsx(SectionTitle,{children:\"Performance Comparison\"}),/*#__PURE__*/_jsxs(ChartPlaceholder,{children:[\"Performance comparison chart will be displayed here\",/*#__PURE__*/_jsx(\"br\",{}),\"(Can integrate Chart.js or Recharts for visualization)\"]}),/*#__PURE__*/_jsxs(InsightBox,{children:[/*#__PURE__*/_jsx(InsightTitle,{children:\"Key Insight\"}),/*#__PURE__*/_jsx(InsightText,{children:\"Our research shows that while LLMs as judges demonstrate impressive capabilities, systematic biases still need to be addressed. We recommend using multi-model voting or human-AI collaboration in practical applications to improve judgment reliability.\"})]}),/*#__PURE__*/_jsx(SectionTitle,{children:\"Detailed Results by Category\"}),/*#__PURE__*/_jsxs(ChartPlaceholder,{children:[\"Detailed breakdown by task category\",/*#__PURE__*/_jsx(\"br\",{}),\"(Detailed results analysis by task categories)\"]})]})}),/*#__PURE__*/_jsx(Footer,{})]});};","map":{"version":3,"names":["React","AnimationRevealPage","styled","Header","Footer","Container","ContentWithPaddingXl","SectionHeading","Subheading","SubheadingBase","jsx","_jsx","jsxs","_jsxs","HeadingRow","tw","div","Heading","Description","p","SectionTitle","h2","FindingsGrid","FindingCard","FindingTitle","h3","FindingText","ChartPlaceholder","InsightBox","InsightTitle","h4","InsightText","MetricRow","MetricCard","_templateObject","_taggedTemplateLiteral","MetricValue","MetricLabel","children"],"sources":["/Users/vicayu/Desktop/Judging-the-Judges.github.io/src/pages/Results.js"],"sourcesContent":["import React from \"react\";\nimport AnimationRevealPage from \"helpers/AnimationRevealPage.js\";\nimport tw from \"twin.macro\";\nimport styled from \"styled-components\";\nimport Header from \"components/headers/light.js\";\nimport Footer from \"components/footers/MiniCenteredFooter.js\";\nimport { Container, ContentWithPaddingXl } from \"components/misc/Layouts\";\nimport { SectionHeading, Subheading as SubheadingBase } from \"components/misc/Headings\";\n\nconst HeadingRow = tw.div`flex flex-col items-center`;\nconst Heading = tw(SectionHeading)`text-gray-900`;\nconst Subheading = tw(SubheadingBase)`text-primary-500 mb-4`;\nconst Description = tw.p`mt-4 text-gray-600 text-center max-w-2xl`;\n\nconst SectionTitle = tw.h2`text-2xl font-bold text-gray-900 mt-16 mb-6`;\n\nconst FindingsGrid = tw.div`grid grid-cols-1 md:grid-cols-2 gap-8 mt-8`;\nconst FindingCard = tw.div`bg-white rounded-xl shadow-lg p-6 border-t-4 border-primary-500`;\nconst FindingTitle = tw.h3`text-lg font-bold text-gray-900 mb-3`;\nconst FindingText = tw.p`text-gray-600 leading-relaxed`;\n\nconst ChartPlaceholder = tw.div`bg-gray-100 rounded-xl p-8 flex items-center justify-center text-gray-500 h-64 mt-8`;\n\nconst InsightBox = tw.div`bg-primary-100 rounded-xl p-6 mt-8`;\nconst InsightTitle = tw.h4`text-primary-700 font-bold mb-2`;\nconst InsightText = tw.p`text-primary-600`;\n\nconst MetricRow = tw.div`flex flex-wrap gap-6 mt-8 justify-center`;\nconst MetricCard = styled.div`\n  ${tw`bg-white rounded-xl shadow p-6 text-center min-w-[200px]`}\n`;\nconst MetricValue = tw.div`text-3xl font-bold text-primary-500`;\nconst MetricLabel = tw.div`text-gray-600 mt-2`;\n\nexport default () => {\n  return (\n    <AnimationRevealPage>\n      <Header />\n      <Container>\n        <ContentWithPaddingXl>\n          <HeadingRow>\n            <Subheading>Findings</Subheading>\n            <Heading>Results & Analysis</Heading>\n            <Description>\n              Comprehensive evaluation results and key findings from our LLM-as-a-Judge research.\n            </Description>\n          </HeadingRow>\n\n          <MetricRow>\n            <MetricCard>\n              <MetricValue>87.3%</MetricValue>\n              <MetricLabel>Average Accuracy</MetricLabel>\n            </MetricCard>\n            <MetricCard>\n              <MetricValue>15.2%</MetricValue>\n              <MetricLabel>Position Bias Rate</MetricLabel>\n            </MetricCard>\n            <MetricCard>\n              <MetricValue>0.82</MetricValue>\n              <MetricLabel>Inter-model Agreement</MetricLabel>\n            </MetricCard>\n            <MetricCard>\n              <MetricValue>23%</MetricValue>\n              <MetricLabel>Length Bias Impact</MetricLabel>\n            </MetricCard>\n          </MetricRow>\n\n          <SectionTitle>Key Findings</SectionTitle>\n          <FindingsGrid>\n            <FindingCard>\n              <FindingTitle>Position Bias</FindingTitle>\n              <FindingText>\n                Most LLM judges exhibit position bias, showing preference for responses \n                appearing in certain positions. GPT-4 and Claude-3 demonstrate the most \n                stability in this regard.\n              </FindingText>\n            </FindingCard>\n            <FindingCard>\n              <FindingTitle>Length Preference</FindingTitle>\n              <FindingText>\n                Longer responses generally receive higher scores, even when content \n                quality is similar. This bias is particularly pronounced when evaluating \n                creative writing tasks.\n              </FindingText>\n            </FindingCard>\n            <FindingCard>\n              <FindingTitle>Self-Enhancement Bias</FindingTitle>\n              <FindingText>\n                LLM judges tend to give higher scores to content they generated themselves. \n                This self-enhancement bias reaches up to 12% in some models.\n              </FindingText>\n            </FindingCard>\n            <FindingCard>\n              <FindingTitle>Consistency</FindingTitle>\n              <FindingText>\n                When repeatedly evaluating the same content, consistency varies significantly \n                across models. Top-tier models achieve over 90% self-consistency.\n              </FindingText>\n            </FindingCard>\n          </FindingsGrid>\n\n          <SectionTitle>Performance Comparison</SectionTitle>\n          <ChartPlaceholder>\n            Performance comparison chart will be displayed here\n            <br />\n            (Can integrate Chart.js or Recharts for visualization)\n          </ChartPlaceholder>\n\n          <InsightBox>\n            <InsightTitle>Key Insight</InsightTitle>\n            <InsightText>\n              Our research shows that while LLMs as judges demonstrate impressive capabilities, \n              systematic biases still need to be addressed. We recommend using multi-model voting \n              or human-AI collaboration in practical applications to improve judgment reliability.\n            </InsightText>\n          </InsightBox>\n\n          <SectionTitle>Detailed Results by Category</SectionTitle>\n          <ChartPlaceholder>\n            Detailed breakdown by task category\n            <br />\n            (Detailed results analysis by task categories)\n          </ChartPlaceholder>\n        </ContentWithPaddingXl>\n      </Container>\n      <Footer />\n    </AnimationRevealPage>\n  );\n};\n"],"mappings":"gNAAA,MAAO,CAAAA,KAAK,KAAM,OAAO,CACzB,MAAO,CAAAC,mBAAmB,KAAM,gCAAgC,CAEhE,MAAO,CAAAC,MAAM,KAAM,mBAAmB,CACtC,MAAO,CAAAC,MAAM,KAAM,6BAA6B,CAChD,MAAO,CAAAC,MAAM,KAAM,0CAA0C,CAC7D,OAASC,SAAS,CAAEC,oBAAoB,KAAQ,yBAAyB,CACzE,OAASC,cAAc,CAAEC,UAAU,GAAI,CAAAC,cAAc,KAAQ,0BAA0B,CAAC,OAAAC,GAAA,IAAAC,IAAA,CAAAC,IAAA,IAAAC,KAAA,yBAExF,KAAM,CAAAC,UAAU,CAAGC,MAAE,CAACC,GAAG,iEAA2B,EAAC,CACrD,KAAM,CAAAC,OAAO,CAAGF,MAAE,CAACR,cAAc,CAAC,0EAAc,EAAC,CACjD,KAAM,CAAAC,UAAU,CAAGO,MAAE,CAACN,cAAc,CAAC,kGAAsB,EAAC,CAC5D,KAAM,CAAAS,WAAW,CAAGH,MAAE,CAACI,CAAC,wIAAyC,EAAC,CAElE,KAAM,CAAAC,YAAY,CAAGL,MAAE,CAACM,EAAE,4JAA4C,EAAC,CAEvE,KAAM,CAAAC,YAAY,CAAGP,MAAE,CAACC,GAAG,oLAA2C,EAAC,CACvE,KAAM,CAAAO,WAAW,CAAGR,MAAE,CAACC,GAAG,ohBAAgE,EAAC,CAC3F,KAAM,CAAAQ,YAAY,CAAGT,MAAE,CAACU,EAAE,4IAAqC,EAAC,CAChE,KAAM,CAAAC,WAAW,CAAGX,MAAE,CAACI,CAAC,kGAA8B,EAAC,CAEvD,KAAM,CAAAQ,gBAAgB,CAAGZ,MAAE,CAACC,GAAG,8SAAoF,EAAC,CAEpH,KAAM,CAAAY,UAAU,CAAGb,MAAE,CAACC,GAAG,kJAAmC,EAAC,CAC7D,KAAM,CAAAa,YAAY,CAAGd,MAAE,CAACe,EAAE,sHAAgC,EAAC,CAC3D,KAAM,CAAAC,WAAW,CAAGhB,MAAE,CAACI,CAAC,2EAAiB,EAAC,CAE1C,KAAM,CAAAa,SAAS,CAAGjB,MAAE,CAACC,GAAG,gGAAyC,EAAC,CAClE,KAAM,CAAAiB,UAAU,CAAG/B,MAAM,CAACc,GAAG,CAAAkB,eAAA,GAAAA,eAAA,CAAAC,sBAAA,iBACvB,+bAAyD,CAAC,CAC/D,CACD,KAAM,CAAAC,WAAW,CAAGrB,MAAE,CAACC,GAAG,qHAAoC,EAAC,CAC/D,KAAM,CAAAqB,WAAW,CAAGtB,MAAE,CAACC,GAAG,kGAAmB,EAAC,CAE9C,cAAe,IAAM,CACnB,mBACEH,KAAA,CAACZ,mBAAmB,EAAAqC,QAAA,eAClB3B,IAAA,CAACR,MAAM,GAAE,CAAC,cACVQ,IAAA,CAACN,SAAS,EAAAiC,QAAA,cACRzB,KAAA,CAACP,oBAAoB,EAAAgC,QAAA,eACnBzB,KAAA,CAACC,UAAU,EAAAwB,QAAA,eACT3B,IAAA,CAACH,UAAU,EAAA8B,QAAA,CAAC,UAAQ,CAAY,CAAC,cACjC3B,IAAA,CAACM,OAAO,EAAAqB,QAAA,CAAC,oBAAkB,CAAS,CAAC,cACrC3B,IAAA,CAACO,WAAW,EAAAoB,QAAA,CAAC,qFAEb,CAAa,CAAC,EACJ,CAAC,cAEbzB,KAAA,CAACmB,SAAS,EAAAM,QAAA,eACRzB,KAAA,CAACoB,UAAU,EAAAK,QAAA,eACT3B,IAAA,CAACyB,WAAW,EAAAE,QAAA,CAAC,OAAK,CAAa,CAAC,cAChC3B,IAAA,CAAC0B,WAAW,EAAAC,QAAA,CAAC,kBAAgB,CAAa,CAAC,EACjC,CAAC,cACbzB,KAAA,CAACoB,UAAU,EAAAK,QAAA,eACT3B,IAAA,CAACyB,WAAW,EAAAE,QAAA,CAAC,OAAK,CAAa,CAAC,cAChC3B,IAAA,CAAC0B,WAAW,EAAAC,QAAA,CAAC,oBAAkB,CAAa,CAAC,EACnC,CAAC,cACbzB,KAAA,CAACoB,UAAU,EAAAK,QAAA,eACT3B,IAAA,CAACyB,WAAW,EAAAE,QAAA,CAAC,MAAI,CAAa,CAAC,cAC/B3B,IAAA,CAAC0B,WAAW,EAAAC,QAAA,CAAC,uBAAqB,CAAa,CAAC,EACtC,CAAC,cACbzB,KAAA,CAACoB,UAAU,EAAAK,QAAA,eACT3B,IAAA,CAACyB,WAAW,EAAAE,QAAA,CAAC,KAAG,CAAa,CAAC,cAC9B3B,IAAA,CAAC0B,WAAW,EAAAC,QAAA,CAAC,oBAAkB,CAAa,CAAC,EACnC,CAAC,EACJ,CAAC,cAEZ3B,IAAA,CAACS,YAAY,EAAAkB,QAAA,CAAC,cAAY,CAAc,CAAC,cACzCzB,KAAA,CAACS,YAAY,EAAAgB,QAAA,eACXzB,KAAA,CAACU,WAAW,EAAAe,QAAA,eACV3B,IAAA,CAACa,YAAY,EAAAc,QAAA,CAAC,eAAa,CAAc,CAAC,cAC1C3B,IAAA,CAACe,WAAW,EAAAY,QAAA,CAAC,2KAIb,CAAa,CAAC,EACH,CAAC,cACdzB,KAAA,CAACU,WAAW,EAAAe,QAAA,eACV3B,IAAA,CAACa,YAAY,EAAAc,QAAA,CAAC,mBAAiB,CAAc,CAAC,cAC9C3B,IAAA,CAACe,WAAW,EAAAY,QAAA,CAAC,sKAIb,CAAa,CAAC,EACH,CAAC,cACdzB,KAAA,CAACU,WAAW,EAAAe,QAAA,eACV3B,IAAA,CAACa,YAAY,EAAAc,QAAA,CAAC,uBAAqB,CAAc,CAAC,cAClD3B,IAAA,CAACe,WAAW,EAAAY,QAAA,CAAC,0IAGb,CAAa,CAAC,EACH,CAAC,cACdzB,KAAA,CAACU,WAAW,EAAAe,QAAA,eACV3B,IAAA,CAACa,YAAY,EAAAc,QAAA,CAAC,aAAW,CAAc,CAAC,cACxC3B,IAAA,CAACe,WAAW,EAAAY,QAAA,CAAC,iJAGb,CAAa,CAAC,EACH,CAAC,EACF,CAAC,cAEf3B,IAAA,CAACS,YAAY,EAAAkB,QAAA,CAAC,wBAAsB,CAAc,CAAC,cACnDzB,KAAA,CAACc,gBAAgB,EAAAW,QAAA,EAAC,qDAEhB,cAAA3B,IAAA,QAAK,CAAC,yDAER,EAAkB,CAAC,cAEnBE,KAAA,CAACe,UAAU,EAAAU,QAAA,eACT3B,IAAA,CAACkB,YAAY,EAAAS,QAAA,CAAC,aAAW,CAAc,CAAC,cACxC3B,IAAA,CAACoB,WAAW,EAAAO,QAAA,CAAC,4PAIb,CAAa,CAAC,EACJ,CAAC,cAEb3B,IAAA,CAACS,YAAY,EAAAkB,QAAA,CAAC,8BAA4B,CAAc,CAAC,cACzDzB,KAAA,CAACc,gBAAgB,EAAAW,QAAA,EAAC,qCAEhB,cAAA3B,IAAA,QAAK,CAAC,iDAER,EAAkB,CAAC,EACC,CAAC,CACd,CAAC,cACZA,IAAA,CAACP,MAAM,GAAE,CAAC,EACS,CAAC,CAE1B,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module"}