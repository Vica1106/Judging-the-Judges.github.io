{"ast":null,"code":"import _taggedTemplateLiteral from\"/Users/vicayu/Desktop/Judging-the-Judges.github.io/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/taggedTemplateLiteral.js\";var _templateObject;import React,{useState}from\"react\";import AnimationRevealPage from\"helpers/AnimationRevealPage.js\";import styled from\"styled-components\";import Header from\"components/headers/light.js\";import Footer from\"components/footers/MiniCenteredFooter.js\";import{Container,ContentWithPaddingXl}from\"components/misc/Layouts\";import{SectionHeading,Subheading as SubheadingBase}from\"components/misc/Headings\";import{jsx as _jsx,jsxs as _jsxs}from\"react/jsx-runtime\";const HeadingRow=styled.div({\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\"});const Heading=styled(SectionHeading)({\"--tw-text-opacity\":\"1\",\"color\":\"rgb(26 32 44 / var(--tw-text-opacity))\"});const Subheading=styled(SubheadingBase)({\"marginBottom\":\"1rem\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(100 21 255 / var(--tw-text-opacity))\"});const Description=styled.p({\"marginTop\":\"1rem\",\"maxWidth\":\"42rem\",\"textAlign\":\"center\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(113 128 150 / var(--tw-text-opacity))\"});const TabsContainer=styled.div({\"marginTop\":\"2.5rem\",\"display\":\"flex\",\"flexWrap\":\"wrap\",\"justifyContent\":\"center\",\"gap\":\"0.5rem\"});const Tab=styled.button(_templateObject||(_templateObject=_taggedTemplateLiteral([\"\\n  \",\"\\n  \",\"\\n\"])),{\"borderRadius\":\"0.5rem\",\"paddingLeft\":\"1.5rem\",\"paddingRight\":\"1.5rem\",\"paddingTop\":\"0.75rem\",\"paddingBottom\":\"0.75rem\",\"fontWeight\":\"600\",\"transitionProperty\":\"background-color, border-color, color, fill, stroke, opacity, box-shadow, transform\",\"transitionDuration\":\"300ms\"},props=>props.active?{\"--tw-bg-opacity\":\"1\",\"backgroundColor\":\"rgb(100 21 255 / var(--tw-bg-opacity))\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(255 255 255 / var(--tw-text-opacity))\"}:{\"--tw-bg-opacity\":\"1\",\"backgroundColor\":\"rgb(247 250 252 / var(--tw-bg-opacity))\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(113 128 150 / var(--tw-text-opacity))\",\":hover\":{\"--tw-bg-opacity\":\"1\",\"backgroundColor\":\"rgb(237 242 247 / var(--tw-bg-opacity))\"}});const ContentSection=styled.div({\"marginTop\":\"3rem\"});const PromptCard=styled.div({\"marginBottom\":\"1.5rem\",\"borderRadius\":\"0.5rem\",\"borderLeftWidth\":\"4px\",\"--tw-border-opacity\":\"1\",\"borderColor\":\"rgb(100 21 255 / var(--tw-border-opacity))\",\"--tw-bg-opacity\":\"1\",\"backgroundColor\":\"rgb(255 255 255 / var(--tw-bg-opacity))\",\"padding\":\"1.5rem\",\"--tw-shadow\":\"0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05)\",\"--tw-shadow-colored\":\"0 10px 15px -3px var(--tw-shadow-color), 0 4px 6px -2px var(--tw-shadow-color)\",\"boxShadow\":\"var(--tw-ring-offset-shadow, 0 0 #0000), var(--tw-ring-shadow, 0 0 #0000), var(--tw-shadow)\"});const PromptTitle=styled.h3({\"marginBottom\":\"0.75rem\",\"fontSize\":\"1.25rem\",\"fontWeight\":\"700\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(26 32 44 / var(--tw-text-opacity))\"});const PromptDescription=styled.p({\"marginBottom\":\"1rem\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(113 128 150 / var(--tw-text-opacity))\"});const PromptCode=styled.pre({\"overflowX\":\"auto\",\"borderRadius\":\"0.5rem\",\"--tw-bg-opacity\":\"1\",\"backgroundColor\":\"rgb(45 55 72 / var(--tw-bg-opacity))\",\"padding\":\"1rem\",\"fontSize\":\"0.875rem\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(104 211 145 / var(--tw-text-opacity))\"});const ExampleContainer=styled.div({\"marginTop\":\"1rem\",\"borderRadius\":\"0.5rem\",\"--tw-bg-opacity\":\"1\",\"backgroundColor\":\"rgb(247 250 252 / var(--tw-bg-opacity))\",\"padding\":\"1rem\"});const ExampleLabel=styled.span({\"fontSize\":\"0.75rem\",\"fontWeight\":\"700\",\"textTransform\":\"uppercase\",\"letterSpacing\":\"0.025em\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(100 21 255 / var(--tw-text-opacity))\"});const ExampleText=styled.p({\"marginTop\":\"0.5rem\",\"--tw-text-opacity\":\"1\",\"color\":\"rgb(74 85 104 / var(--tw-text-opacity))\"});const categories=[\"All\",\"Evaluation\",\"Comparison\",\"Scoring\",\"Analysis\"];const promptsData=[{category:\"Evaluation\",title:\"Basic Evaluation Prompt\",description:\"A foundational prompt for evaluating the quality of a single response.\",prompt:\"You are an expert evaluator. Please evaluate the following response based on:\\n1. Accuracy - Is the information correct?\\n2. Completeness - Does it fully address the question?\\n3. Clarity - Is it well-organized and easy to understand?\\n\\nResponse to evaluate:\\n{response}\\n\\nPlease provide a score from 1-10 and explain your reasoning.\",example:{input:\"What is machine learning?\",response:\"Machine learning is a subset of AI that enables systems to learn from data.\"}},{category:\"Comparison\",title:\"Pairwise Comparison Prompt\",description:\"Used for comparing two responses and selecting the better one.\",prompt:\"Compare the following two responses and determine which one is better.\\n\\nResponse A:\\n{response_a}\\n\\nResponse B:\\n{response_b}\\n\\nConsider: accuracy, helpfulness, and clarity.\\nOutput: \\\"A\\\" or \\\"B\\\" with explanation.\",example:{input:\"Compare responses about climate change\",response:\"Response A provides more scientific evidence...\"}},{category:\"Scoring\",title:\"Multi-Dimension Scoring Prompt\",description:\"A multi-dimensional scoring prompt for fine-grained evaluation.\",prompt:\"Evaluate the response on the following dimensions (1-5 scale each):\\n\\n1. Factual Accuracy: [score]\\n2. Logical Coherence: [score]  \\n3. Language Quality: [score]\\n4. Relevance: [score]\\n5. Depth of Analysis: [score]\\n\\nResponse:\\n{response}\\n\\nProvide scores and brief justifications for each dimension.\",example:{input:\"Rate this technical explanation\",response:\"Factual: 4/5, Coherence: 5/5, Language: 4/5...\"}},{category:\"Analysis\",title:\"Bias Detection Prompt\",description:\"A prompt for detecting potential biases in evaluations.\",prompt:\"Analyze the following evaluation for potential biases:\\n\\nOriginal Question: {question}\\nResponse: {response}\\nEvaluation: {evaluation}\\n\\nCheck for:\\n- Position bias\\n- Length bias\\n- Style preference bias\\n- Factual vs. opinion bias\\n\\nReport any detected biases with examples.\",example:{input:\"Check for bias in evaluation\",response:\"Detected: Length bias - longer response rated higher...\"}}];export default()=>{const[activeTab,setActiveTab]=useState(\"All\");const filteredPrompts=activeTab===\"All\"?promptsData:promptsData.filter(p=>p.category===activeTab);return/*#__PURE__*/_jsxs(AnimationRevealPage,{children:[/*#__PURE__*/_jsx(Header,{}),/*#__PURE__*/_jsx(Container,{children:/*#__PURE__*/_jsxs(ContentWithPaddingXl,{children:[/*#__PURE__*/_jsxs(HeadingRow,{children:[/*#__PURE__*/_jsx(Subheading,{children:\"Resources\"}),/*#__PURE__*/_jsx(Heading,{children:\"Prompts & Examples\"}),/*#__PURE__*/_jsx(Description,{children:\"The evaluation prompts and examples we use in our research. These prompts are carefully designed to test various capabilities of LLMs as judges.\"})]}),/*#__PURE__*/_jsx(TabsContainer,{children:categories.map(cat=>/*#__PURE__*/_jsx(Tab,{active:activeTab===cat,onClick:()=>setActiveTab(cat),children:cat},cat))}),/*#__PURE__*/_jsx(ContentSection,{children:filteredPrompts.map((item,index)=>/*#__PURE__*/_jsxs(PromptCard,{children:[/*#__PURE__*/_jsx(PromptTitle,{children:item.title}),/*#__PURE__*/_jsx(PromptDescription,{children:item.description}),/*#__PURE__*/_jsx(PromptCode,{children:item.prompt}),/*#__PURE__*/_jsxs(ExampleContainer,{children:[/*#__PURE__*/_jsx(ExampleLabel,{children:\"Example Usage\"}),/*#__PURE__*/_jsxs(ExampleText,{children:[/*#__PURE__*/_jsx(\"strong\",{children:\"Input:\"}),\" \",item.example.input]}),/*#__PURE__*/_jsxs(ExampleText,{children:[/*#__PURE__*/_jsx(\"strong\",{children:\"Expected Output:\"}),\" \",item.example.response]})]})]},index))})]})}),/*#__PURE__*/_jsx(Footer,{})]});};","map":{"version":3,"names":["React","useState","AnimationRevealPage","styled","Header","Footer","Container","ContentWithPaddingXl","SectionHeading","Subheading","SubheadingBase","jsx","_jsx","jsxs","_jsxs","HeadingRow","tw","div","Heading","Description","p","TabsContainer","Tab","button","_templateObject","_taggedTemplateLiteral","props","active","ContentSection","PromptCard","PromptTitle","h3","PromptDescription","PromptCode","pre","ExampleContainer","ExampleLabel","span","ExampleText","categories","promptsData","category","title","description","prompt","example","input","response","activeTab","setActiveTab","filteredPrompts","filter","children","map","cat","onClick","item","index"],"sources":["/Users/vicayu/Desktop/Judging-the-Judges.github.io/src/pages/Prompts.js"],"sourcesContent":["import React, { useState } from \"react\";\nimport AnimationRevealPage from \"helpers/AnimationRevealPage.js\";\nimport tw from \"twin.macro\";\nimport styled from \"styled-components\";\nimport Header from \"components/headers/light.js\";\nimport Footer from \"components/footers/MiniCenteredFooter.js\";\nimport { Container, ContentWithPaddingXl } from \"components/misc/Layouts\";\nimport { SectionHeading, Subheading as SubheadingBase } from \"components/misc/Headings\";\n\nconst HeadingRow = tw.div`flex flex-col items-center`;\nconst Heading = tw(SectionHeading)`text-gray-900`;\nconst Subheading = tw(SubheadingBase)`text-primary-500 mb-4`;\nconst Description = tw.p`mt-4 text-gray-600 text-center max-w-2xl`;\n\nconst TabsContainer = tw.div`mt-10 flex flex-wrap justify-center gap-2`;\nconst Tab = styled.button`\n  ${tw`px-6 py-3 rounded-lg font-semibold transition duration-300`}\n  ${props => props.active \n    ? tw`bg-primary-500 text-white` \n    : tw`bg-gray-100 text-gray-600 hover:bg-gray-200`}\n`;\n\nconst ContentSection = tw.div`mt-12`;\nconst PromptCard = tw.div`bg-white rounded-lg shadow-lg p-6 mb-6 border-l-4 border-primary-500`;\nconst PromptTitle = tw.h3`text-xl font-bold text-gray-900 mb-3`;\nconst PromptDescription = tw.p`text-gray-600 mb-4`;\nconst PromptCode = tw.pre`bg-gray-800 text-green-400 p-4 rounded-lg overflow-x-auto text-sm`;\n\nconst ExampleContainer = tw.div`mt-4 bg-gray-100 p-4 rounded-lg`;\nconst ExampleLabel = tw.span`text-xs font-bold text-primary-500 uppercase tracking-wide`;\nconst ExampleText = tw.p`mt-2 text-gray-700`;\n\nconst categories = [\"All\", \"Evaluation\", \"Comparison\", \"Scoring\", \"Analysis\"];\n\nconst promptsData = [\n  {\n    category: \"Evaluation\",\n    title: \"Basic Evaluation Prompt\",\n    description: \"A foundational prompt for evaluating the quality of a single response.\",\n    prompt: `You are an expert evaluator. Please evaluate the following response based on:\n1. Accuracy - Is the information correct?\n2. Completeness - Does it fully address the question?\n3. Clarity - Is it well-organized and easy to understand?\n\nResponse to evaluate:\n{response}\n\nPlease provide a score from 1-10 and explain your reasoning.`,\n    example: {\n      input: \"What is machine learning?\",\n      response: \"Machine learning is a subset of AI that enables systems to learn from data.\"\n    }\n  },\n  {\n    category: \"Comparison\",\n    title: \"Pairwise Comparison Prompt\",\n    description: \"Used for comparing two responses and selecting the better one.\",\n    prompt: `Compare the following two responses and determine which one is better.\n\nResponse A:\n{response_a}\n\nResponse B:\n{response_b}\n\nConsider: accuracy, helpfulness, and clarity.\nOutput: \"A\" or \"B\" with explanation.`,\n    example: {\n      input: \"Compare responses about climate change\",\n      response: \"Response A provides more scientific evidence...\"\n    }\n  },\n  {\n    category: \"Scoring\",\n    title: \"Multi-Dimension Scoring Prompt\",\n    description: \"A multi-dimensional scoring prompt for fine-grained evaluation.\",\n    prompt: `Evaluate the response on the following dimensions (1-5 scale each):\n\n1. Factual Accuracy: [score]\n2. Logical Coherence: [score]  \n3. Language Quality: [score]\n4. Relevance: [score]\n5. Depth of Analysis: [score]\n\nResponse:\n{response}\n\nProvide scores and brief justifications for each dimension.`,\n    example: {\n      input: \"Rate this technical explanation\",\n      response: \"Factual: 4/5, Coherence: 5/5, Language: 4/5...\"\n    }\n  },\n  {\n    category: \"Analysis\",\n    title: \"Bias Detection Prompt\",\n    description: \"A prompt for detecting potential biases in evaluations.\",\n    prompt: `Analyze the following evaluation for potential biases:\n\nOriginal Question: {question}\nResponse: {response}\nEvaluation: {evaluation}\n\nCheck for:\n- Position bias\n- Length bias\n- Style preference bias\n- Factual vs. opinion bias\n\nReport any detected biases with examples.`,\n    example: {\n      input: \"Check for bias in evaluation\",\n      response: \"Detected: Length bias - longer response rated higher...\"\n    }\n  }\n];\n\nexport default () => {\n  const [activeTab, setActiveTab] = useState(\"All\");\n\n  const filteredPrompts = activeTab === \"All\" \n    ? promptsData \n    : promptsData.filter(p => p.category === activeTab);\n\n  return (\n    <AnimationRevealPage>\n      <Header />\n      <Container>\n        <ContentWithPaddingXl>\n          <HeadingRow>\n            <Subheading>Resources</Subheading>\n            <Heading>Prompts & Examples</Heading>\n            <Description>\n              The evaluation prompts and examples we use in our research. \n              These prompts are carefully designed to test various capabilities of LLMs as judges.\n            </Description>\n          </HeadingRow>\n\n          <TabsContainer>\n            {categories.map(cat => (\n              <Tab \n                key={cat}\n                active={activeTab === cat}\n                onClick={() => setActiveTab(cat)}\n              >\n                {cat}\n              </Tab>\n            ))}\n          </TabsContainer>\n\n          <ContentSection>\n            {filteredPrompts.map((item, index) => (\n              <PromptCard key={index}>\n                <PromptTitle>{item.title}</PromptTitle>\n                <PromptDescription>{item.description}</PromptDescription>\n                <PromptCode>{item.prompt}</PromptCode>\n                <ExampleContainer>\n                  <ExampleLabel>Example Usage</ExampleLabel>\n                  <ExampleText><strong>Input:</strong> {item.example.input}</ExampleText>\n                  <ExampleText><strong>Expected Output:</strong> {item.example.response}</ExampleText>\n                </ExampleContainer>\n              </PromptCard>\n            ))}\n          </ContentSection>\n        </ContentWithPaddingXl>\n      </Container>\n      <Footer />\n    </AnimationRevealPage>\n  );\n};\n"],"mappings":"gNAAA,MAAO,CAAAA,KAAK,EAAIC,QAAQ,KAAQ,OAAO,CACvC,MAAO,CAAAC,mBAAmB,KAAM,gCAAgC,CAEhE,MAAO,CAAAC,MAAM,KAAM,mBAAmB,CACtC,MAAO,CAAAC,MAAM,KAAM,6BAA6B,CAChD,MAAO,CAAAC,MAAM,KAAM,0CAA0C,CAC7D,OAASC,SAAS,CAAEC,oBAAoB,KAAQ,yBAAyB,CACzE,OAASC,cAAc,CAAEC,UAAU,GAAI,CAAAC,cAAc,KAAQ,0BAA0B,CAAC,OAAAC,GAAA,IAAAC,IAAA,CAAAC,IAAA,IAAAC,KAAA,yBAExF,KAAM,CAAAC,UAAU,CAAGC,MAAE,CAACC,GAAG,iEAA2B,EAAC,CACrD,KAAM,CAAAC,OAAO,CAAGF,MAAE,CAACR,cAAc,CAAC,0EAAc,EAAC,CACjD,KAAM,CAAAC,UAAU,CAAGO,MAAE,CAACN,cAAc,CAAC,kGAAsB,EAAC,CAC5D,KAAM,CAAAS,WAAW,CAAGH,MAAE,CAACI,CAAC,wIAAyC,EAAC,CAElE,KAAM,CAAAC,aAAa,CAAGL,MAAE,CAACC,GAAG,kGAA0C,EAAC,CACvE,KAAM,CAAAK,GAAG,CAAGnB,MAAM,CAACoB,MAAM,CAAAC,eAAA,GAAAA,eAAA,CAAAC,sBAAA,wBACnB,mRAA2D,CAAC,CAC9DC,KAAK,EAAIA,KAAK,CAACC,MAAM,CACjB,6JAA0B,CAAC,CAC3B,2PAA4C,CAAC,CACpD,CAED,KAAM,CAAAC,cAAc,CAAGZ,MAAE,CAACC,GAAG,oBAAM,EAAC,CACpC,KAAM,CAAAY,UAAU,CAAGb,MAAE,CAACC,GAAG,4iBAAqE,EAAC,CAC/F,KAAM,CAAAa,WAAW,CAAGd,MAAE,CAACe,EAAE,2IAAqC,EAAC,CAC/D,KAAM,CAAAC,iBAAiB,CAAGhB,MAAE,CAACI,CAAC,mGAAmB,EAAC,CAClD,KAAM,CAAAa,UAAU,CAAGjB,MAAE,CAACkB,GAAG,8OAAkE,EAAC,CAE5F,KAAM,CAAAC,gBAAgB,CAAGnB,MAAE,CAACC,GAAG,+IAAgC,EAAC,CAChE,KAAM,CAAAmB,YAAY,CAAGpB,MAAE,CAACqB,IAAI,0KAA2D,EAAC,CACxF,KAAM,CAAAC,WAAW,CAAGtB,MAAE,CAACI,CAAC,gGAAmB,EAAC,CAE5C,KAAM,CAAAmB,UAAU,CAAG,CAAC,KAAK,CAAE,YAAY,CAAE,YAAY,CAAE,SAAS,CAAE,UAAU,CAAC,CAE7E,KAAM,CAAAC,WAAW,CAAG,CAClB,CACEC,QAAQ,CAAE,YAAY,CACtBC,KAAK,CAAE,yBAAyB,CAChCC,WAAW,CAAE,wEAAwE,CACrFC,MAAM,kVAQmD,CACzDC,OAAO,CAAE,CACPC,KAAK,CAAE,2BAA2B,CAClCC,QAAQ,CAAE,6EACZ,CACF,CAAC,CACD,CACEN,QAAQ,CAAE,YAAY,CACtBC,KAAK,CAAE,4BAA4B,CACnCC,WAAW,CAAE,gEAAgE,CAC7EC,MAAM,8NAS2B,CACjCC,OAAO,CAAE,CACPC,KAAK,CAAE,wCAAwC,CAC/CC,QAAQ,CAAE,iDACZ,CACF,CAAC,CACD,CACEN,QAAQ,CAAE,SAAS,CACnBC,KAAK,CAAE,gCAAgC,CACvCC,WAAW,CAAE,iEAAiE,CAC9EC,MAAM,mTAWkD,CACxDC,OAAO,CAAE,CACPC,KAAK,CAAE,iCAAiC,CACxCC,QAAQ,CAAE,gDACZ,CACF,CAAC,CACD,CACEN,QAAQ,CAAE,UAAU,CACpBC,KAAK,CAAE,uBAAuB,CAC9BC,WAAW,CAAE,yDAAyD,CACtEC,MAAM,0RAYgC,CACtCC,OAAO,CAAE,CACPC,KAAK,CAAE,8BAA8B,CACrCC,QAAQ,CAAE,yDACZ,CACF,CAAC,CACF,CAED,cAAe,IAAM,CACnB,KAAM,CAACC,SAAS,CAAEC,YAAY,CAAC,CAAGhD,QAAQ,CAAC,KAAK,CAAC,CAEjD,KAAM,CAAAiD,eAAe,CAAGF,SAAS,GAAK,KAAK,CACvCR,WAAW,CACXA,WAAW,CAACW,MAAM,CAAC/B,CAAC,EAAIA,CAAC,CAACqB,QAAQ,GAAKO,SAAS,CAAC,CAErD,mBACElC,KAAA,CAACZ,mBAAmB,EAAAkD,QAAA,eAClBxC,IAAA,CAACR,MAAM,GAAE,CAAC,cACVQ,IAAA,CAACN,SAAS,EAAA8C,QAAA,cACRtC,KAAA,CAACP,oBAAoB,EAAA6C,QAAA,eACnBtC,KAAA,CAACC,UAAU,EAAAqC,QAAA,eACTxC,IAAA,CAACH,UAAU,EAAA2C,QAAA,CAAC,WAAS,CAAY,CAAC,cAClCxC,IAAA,CAACM,OAAO,EAAAkC,QAAA,CAAC,oBAAkB,CAAS,CAAC,cACrCxC,IAAA,CAACO,WAAW,EAAAiC,QAAA,CAAC,kJAGb,CAAa,CAAC,EACJ,CAAC,cAEbxC,IAAA,CAACS,aAAa,EAAA+B,QAAA,CACXb,UAAU,CAACc,GAAG,CAACC,GAAG,eACjB1C,IAAA,CAACU,GAAG,EAEFK,MAAM,CAAEqB,SAAS,GAAKM,GAAI,CAC1BC,OAAO,CAAEA,CAAA,GAAMN,YAAY,CAACK,GAAG,CAAE,CAAAF,QAAA,CAEhCE,GAAG,EAJCA,GAKF,CACN,CAAC,CACW,CAAC,cAEhB1C,IAAA,CAACgB,cAAc,EAAAwB,QAAA,CACZF,eAAe,CAACG,GAAG,CAAC,CAACG,IAAI,CAAEC,KAAK,gBAC/B3C,KAAA,CAACe,UAAU,EAAAuB,QAAA,eACTxC,IAAA,CAACkB,WAAW,EAAAsB,QAAA,CAAEI,IAAI,CAACd,KAAK,CAAc,CAAC,cACvC9B,IAAA,CAACoB,iBAAiB,EAAAoB,QAAA,CAAEI,IAAI,CAACb,WAAW,CAAoB,CAAC,cACzD/B,IAAA,CAACqB,UAAU,EAAAmB,QAAA,CAAEI,IAAI,CAACZ,MAAM,CAAa,CAAC,cACtC9B,KAAA,CAACqB,gBAAgB,EAAAiB,QAAA,eACfxC,IAAA,CAACwB,YAAY,EAAAgB,QAAA,CAAC,eAAa,CAAc,CAAC,cAC1CtC,KAAA,CAACwB,WAAW,EAAAc,QAAA,eAACxC,IAAA,WAAAwC,QAAA,CAAQ,QAAM,CAAQ,CAAC,IAAC,CAACI,IAAI,CAACX,OAAO,CAACC,KAAK,EAAc,CAAC,cACvEhC,KAAA,CAACwB,WAAW,EAAAc,QAAA,eAACxC,IAAA,WAAAwC,QAAA,CAAQ,kBAAgB,CAAQ,CAAC,IAAC,CAACI,IAAI,CAACX,OAAO,CAACE,QAAQ,EAAc,CAAC,EACpE,CAAC,GARJU,KASL,CACb,CAAC,CACY,CAAC,EACG,CAAC,CACd,CAAC,cACZ7C,IAAA,CAACP,MAAM,GAAE,CAAC,EACS,CAAC,CAE1B,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module"}